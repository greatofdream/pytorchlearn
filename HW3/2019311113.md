# 手写字体识别
张爱强 2019311113

## 设计网络

+ `label`使用0,1,3,4....,49索引表示
+ 优化器选择SGD
+ 损失函数使用CrossEntropy
+ 学习率 0.005
+ 动量 0.9

### 全连接神经网络

仅用三层全连接层组成的网络，accuracy=46.8%，训练速度较快

```python
nn.Sequential(
            nn.Linear(in_features, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512,out_features)
        )
```

### LeNet

模仿`LeNet`的结构设计了一个卷积神经网络，accuracy=68%

```python
NeuralNetwork(
  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1024, out_features=512, bias=True)
  (fc2): Linear(in_features=512, out_features=128, bias=True)
  (fc3): Linear(in_features=128, out_features=50, bias=True)
)
```
对比上一个全连接神经网络，多了两层卷积层

### 基准线

比起`LeNet`多了BatchNorm层和Dropout层，accuracy最好可以到83.6%;

**另外使用Adam拟合器，最好可以到88%**,具体讨论见下一节

```python
NeuralNetwork(
  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))
  (conv4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))
  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_sta
ts=True)
  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_sta
ts=True)
  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_sta
ts=True)
  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_sta
ts=True)
  (bn5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_st
ats=True)
  (bn6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_sta
ts=True)
  (fc1): Linear(in_features=1600, out_features=512, bias=True)
  (fc3): Linear(in_features=512, out_features=128, bias=True)
  (fc2): Linear(in_features=512, out_features=50, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
}
```
对卷积层内部的channel数反复调整，发现只能使得accuracy下降，第一层32channel是最佳的，将channel数调大或调小都不好，很玄学。

对比`LeNet`结构，增加了BatchNorm层和Dropout层，层数增加使得accuracy上升很多。

## 对比
### 模型的影响
将基准线中的BatchNorm层全部去除，再次训练，accuracy最好可以到73%，说明batchNorm的作用还是比较大。

### 超参数的影响
对于基准线模型，调整拟合器为Adam，最终accuracy最好一般可以到88%左右，偶尔可以达到90%;每次训练结果会有变动，最好值偶尔会到86%


### 欠拟合与过拟合
以Adam拟合器为例子，绘制出train过程的loss和test的准确率变化曲线

![](traintest.png)

+ epoch小于150前，处于拟合比较好的状态，epoch=135时，accuracy=88%
+ epoch 大于150时，出现了一个跳跃，拟合器跳到了另外一个局域极值里，或者说到了过拟合的状态

这个问题可以通过选择test数据集上accuracy最好的模型解决